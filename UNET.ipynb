{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentinel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "mount_file_id": "1M8wNE-9r2hAeQ9ns0QrWQqib3HH3SexZ",
      "authorship_tag": "ABX9TyMc3C+cP8zXl3SxWY0YRWsI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nat-D/varuna-hackathon/blob/main/UNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNET Model"
      ],
      "metadata": {
        "id": "Pzo1VRHVdsXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels), \n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels), \n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels=12, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)     \n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part \n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                    nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
        "                )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "\n",
        "    def standardize_per_channel(self, x):\n",
        "        N = x.shape[0]\n",
        "        C = x.shape[1]\n",
        "        x_view = x.view(N,C,-1)\n",
        "        x_mean = torch.mean(x_view, dim=2).view(N,C,1,1)\n",
        "        x_std = 1e-5 + torch.std(x_view, dim=2).view(N,C,1,1)\n",
        "\n",
        "        return (x - x_mean) / x_std\n",
        "\n",
        "    \n",
        "    def standardize_per_sample(self, x):\n",
        "        N = x.shape[0]      \n",
        "        x_view = x.view(N,-1)\n",
        "        x_mean = torch.mean(x_view, dim=1).view(N,1,1,1) \n",
        "        x_std = 1e-5 + torch.std(x_view, dim=1).view(N,1,1,1)\n",
        "\n",
        "        return (x - x_mean) / x_std\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # standardization\n",
        "        x = self.standardize_per_sample(x)\n",
        "\n",
        "        # unet model\n",
        "        skip_connections = []\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x) # the ConvTransposed\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)  #[b,c,h,w]\n",
        "            x = self.ups[idx+1](concat_skip) # the DoubleConv\n",
        "\n",
        "        return self.final_conv(x) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((3, 1, 160, 160))\n",
        "    model = UNET(in_channels=1, out_channels=1)\n",
        "    preds = model(x)\n",
        "    \n",
        "    assert preds.shape == x.shape\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "metadata": {
        "id": "6uCu4q-fdt0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "VruFBguwd074"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Sentinel2(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".npy\", \"_label.npy\"))\n",
        "\n",
        "        image = np.load(img_path).astype(np.float16)\n",
        "        mask = np.load(mask_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def test_sentinel_data():\n",
        "    image_dir = \"/content/drive/MyDrive/data/train/img/\"\n",
        "    mask_dir = \"/content/drive/MyDrive/data/train/mask/\"\n",
        "\n",
        "    sentinel = Sentinel2(image_dir, mask_dir)\n",
        "    img, label = sentinel[0]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_sentinel_data()"
      ],
      "metadata": {
        "id": "luG5BfWgd3YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "EP-x_3qdd6tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    ):\n",
        "\n",
        "    train_ds = Sentinel2(\n",
        "            image_dir=train_dir,\n",
        "            mask_dir=train_maskdir,\n",
        "            transform=train_transform,\n",
        "        )\n",
        "    train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "    val_ds = Sentinel2(\n",
        "            image_dir=val_dir,\n",
        "            mask_dir=val_maskdir,\n",
        "            transform=val_transform,\n",
        "        )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "color_group = np.array([[0,0,0],  # no data\n",
        "                       [244,35,232], \n",
        "                       [250,170,160],\n",
        "                       [230,150,140],\n",
        "                       [0, 255, 0],   #vegetation\n",
        "                       [102,102,156],\n",
        "                       [190,153,153],\n",
        "                       [180,165,180],\n",
        "                       [150,100,100],\n",
        "                       [150,120, 90],\n",
        "                       [153,153,153],\n",
        "                       [1  , 1  ,1 ]\n",
        "                       ])\n",
        "\n",
        "class Logger():\n",
        "    def __init__(self, device=\"cuda\", log_dir='runs'):\n",
        "        \n",
        "        self.writer = SummaryWriter(log_dir)\n",
        "\n",
        "        self.device = device\n",
        "        self.accumulate_training_loss = 0.0\n",
        "        self.training_step = 0\n",
        "        self.epoch_num_step = 0\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def compute_precision(self, true_pos, false_pos, false_neg):\n",
        "        return true_pos / (true_pos + false_pos + 1e-5)\n",
        "\n",
        "    def compute_recall(self, true_pos, false_pos, false_neg):\n",
        "        return true_pos / (false_neg + true_pos + 1e-5)\n",
        "\n",
        "    def compute_iou(self, true_pos, false_pos, false_neg):\n",
        "        return true_pos / (true_pos + false_pos + false_neg + 1e-5)\n",
        "\n",
        "    def compute_weighted_iou():\n",
        "        return\n",
        "\n",
        "\n",
        "    def validation(self, loader, model):\n",
        "        model.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        num_step = 0\n",
        "        total_iou_for_each_class = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x,y in loader:\n",
        "                x = x.to(self.device)\n",
        "                y = y.long().to(self.device)\n",
        "\n",
        "                preds = model(x)\n",
        "                loss = self.loss_fn(preds, y)\n",
        "                \n",
        "                total_loss += loss\n",
        "                num_step += 1\n",
        "\n",
        "                preds_max = torch.argmax(preds, dim=1, keepdim=False) #[B, H, W]\n",
        "                preds_max = F.one_hot(preds_max, num_classes=model.out_channels) #[B, H, W, C]\n",
        "\n",
        "                y = F.one_hot(y, num_classes=model.out_channels) #[B, H, W, C]\n",
        "                \n",
        "                ones = torch.ones_like(preds_max)\n",
        "                zeros = torch.zeros_like(preds_max)\n",
        "\n",
        "                true_pos = torch.logical_and((preds_max == ones), (preds_max == y)) \n",
        "                true_pos = torch.sum(true_pos, dim=(1,2)) # [B, C]\n",
        "\n",
        "                false_pos = torch.logical_and((preds_max == ones),(preds_max != y))\n",
        "                false_pos = torch.sum(false_pos, dim=(1,2))\n",
        "\n",
        "                false_neg = torch.logical_and((preds_max == zeros), (preds_max != y))\n",
        "                false_neg = torch.sum(false_neg, dim=(1,2))\n",
        "                \n",
        "                iou = self.compute_iou(true_pos, false_pos, false_neg) \n",
        "                total_iou_for_each_class += torch.sum(iou, dim=0)\n",
        "                \n",
        "\n",
        "        self.writer.add_scalar(\"Loss/Average_validation_loss\", \n",
        "                                total_loss/num_step, \n",
        "                                self.training_step)\n",
        "\n",
        "        for cls in range(model.out_channels):\n",
        "            self.writer.add_scalar(f\"IoU/Average_iou_class_{cls}\",\n",
        "                                   total_iou_for_each_class[cls]/(num_step * loader.batch_size),\n",
        "                                   self.training_step)\n",
        "\n",
        "        model.train()\n",
        "        return  total_loss/num_step\n",
        "\n",
        "\n",
        "    def save_predictions_as_img(self, loader, model):\n",
        "\n",
        "        model.eval()\n",
        "        \n",
        "        x, y = next(iter(loader))\n",
        "        x = x.to(device=self.device)\n",
        "        with torch.no_grad():\n",
        "            preds = torch.argmax(model(x), dim=1, keepdim=False)\n",
        "            preds_np = preds.cpu().numpy()\n",
        " \n",
        "            for idx in range(preds_np.shape[0]):\n",
        "                \n",
        "                rgb_mask = color_group[preds_np[idx]]\n",
        "                self.writer.add_image(f'predict/{idx}', rgb_mask/255., self.training_step, dataformats=\"HWC\")\n",
        "\n",
        "                rgb_mask_groundtruth = color_group[y[idx]]\n",
        "                self.writer.add_image(f'target/{idx}_mask', rgb_mask_groundtruth/255., self.training_step, dataformats=\"HWC\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    def log_step(self, loss):\n",
        "        self.accumulate_training_loss += loss\n",
        "        self.epoch_num_step += 1\n",
        "        self.training_step += 1\n",
        "\n",
        "    def log_epoch(self, val_loader, model, optimizer):\n",
        "        self.writer.add_scalar('Loss/Average_training_loss',\n",
        "                               self.accumulate_training_loss/ self.epoch_num_step, self.training_step)\n",
        "        \n",
        "        self.accumulate_training_loss = 0\n",
        "        self.epoch_num_step = 0\n",
        "\n",
        "        \n",
        "        if self.training_step % 5 == 0:\n",
        "            self.save_predictions_as_img(val_loader, model)\n",
        "            val_loss = self.validation(val_loader, model)\n",
        "        return val_loss"
      ],
      "metadata": {
        "id": "4cRs9O4ld8ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "tOUwg6WPfRk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==0.4.6"
      ],
      "metadata": {
        "id": "TqEaG4IDgg_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Experiment log\n",
        "LOG_DIR = \"/content/runs/standardize_per_sample_augmentation/\"\n",
        "\n",
        "# Hyper parameters\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 1000\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL= False \n",
        "\n",
        "\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/data/train/img/\"\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/data/train/mask/\"\n",
        "VAL_IMG_DIR = \"/content/drive/MyDrive/data/val/img/\"\n",
        "VAL_MASK_DIR = \"/content/drive/MyDrive/data/val/mask\" \n",
        "\n",
        "\n",
        "\n",
        "def train_fn(epoch, loader, model, optimizer, loss_fn, scaler, logger):\n",
        "\n",
        "    # decorate loader with tqdm\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.long().to(device=DEVICE)\n",
        "\n",
        "        # forward (float16 )\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data) # + 1e-8  # epsilon to prevent loss=nan\n",
        "            loss = loss_fn(predictions, targets) \n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "\n",
        "        # update tqdm loop\n",
        "        loop.set_postfix(epoch=epoch, loss=loss.item())\n",
        "\n",
        "        # log training \n",
        "        logger.log_step(loss.item())\n",
        "\n",
        "def main():\n",
        "    train_transform = A.Compose([\n",
        "            A.ToFloat(max_value=65535.0), # support uint16\n",
        "            # A.RandomResizedCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, p=1.0),\n",
        "            # A.RandomCrop(height=300, width=300, p=0.5),\n",
        "            # A.Rotate(limit= 90, p=0.5), \n",
        "            # A.RandomBrightnessContrast(p=0.5),\n",
        "            # A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.RandomSizedCrop(min_max_height=(100, 400), height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.Transpose(p=0.5),\n",
        "            A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "            A.GridDistortion(p=0.5),\n",
        "            A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=0.5),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    val_transforms = A.Compose([\n",
        "            A.ToFloat(max_value=65535.0),\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    model = UNET(in_channels=12, out_channels=12).to(DEVICE)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.75, patience=30, threshold=0.001, verbose=\"True\")\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        TRAIN_IMG_DIR,\n",
        "        TRAIN_MASK_DIR,\n",
        "        VAL_IMG_DIR,\n",
        "        VAL_MASK_DIR,\n",
        "        BATCH_SIZE,\n",
        "        train_transform,\n",
        "        val_transforms,\n",
        "        NUM_WORKERS,\n",
        "        PIN_MEMORY\n",
        "        )\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
        "\n",
        "    logger = Logger(device=DEVICE, log_dir=LOG_DIR)\n",
        "    \n",
        "\n",
        "    for epoch in range(NUM_EPOCHS): \n",
        "        train_fn(epoch, train_loader, model, optimizer, loss_fn, scaler, logger)\n",
        "        val_loss = logger.log_epoch(val_loader, model, optimizer)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dEOHCXRxfS88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sUgaYcKwgcDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}